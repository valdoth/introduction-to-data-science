{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2810ff-7008-47e2-b9af-4d7ae3662784",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+3\"><strong>Time Series: Statistical Models</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ee28e-b2ae-4140-94f3-18320cd6c480",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Autoregression\n",
    "\n",
    "Autoregression (AR) is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step. AR works in a similar way to **autocorrelation**: in both cases, we're taking data from one part of a set and comparing it to another part. An AR model regresses itself. \n",
    "\n",
    "## Cleaning the Data\n",
    "\n",
    "Just like with linear regression, we'll start by bringing in some tools to help us along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cbcf8-4b37-4775-80de-90381cae651c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pymongo import MongoClient\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64dd800-35cd-4fc6-ab6c-8039e7120fac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Since we'll be working with the `\"air-quality\"` data again, we need to connect to the server, start our client, and grab the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c05154-4475-4640-b67c-6b3c77533ad9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient(host=\"localhost\", port=27017)\n",
    "db = client[\"air-quality\"]\n",
    "lagos = db[\"lagos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8156d-5e20-489f-89a9-b279755094bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Just to make sure we're all on the same page, import all those libraries and get your database up and running. Remember that even though all the examples use the Site 3 data from the `lagos` collection, the practice sets should use Site 4 data from the `lagos` collection. Call your database `lagos_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16011813-c00f-43cd-b397-e13ec7ddb0d9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "lagos_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f9986-0e10-4554-952f-89a8eb7ac279",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In order to get our data into a form we can use to build our model, we're going to need to transform it in several key ways. The first thing we need to do is to get the data we need, and save the results in a DataFrame. Since we're interested in predicting the changes in air quality over time, let's set the DataFrame's index to `\"timestamp\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff06670-cb53-476a-a18f-57a88545208e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "results = lagos.find(\n",
    "    # Note that the `3` refers to Site 3.\n",
    "    {\"metadata.site\": 3, \"metadata.measurement\": \"P2\"},\n",
    "    projection={\"P2\": 1, \"timestamp\": 1, \"_id\": 0},\n",
    ")\n",
    "df = pd.DataFrame(list(results)).set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cab2b8-4477-4373-9d8d-3fa392d26f36",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a list called `results_prac` that pulls data from Site 4 in the `lagos` data, then save it in a DataFrame called `df_prac` with the index `\"timestamp\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7592bd-3f53-4706-83b3-82d09e05c860",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf7691-0bba-4721-974f-75df00f1286d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Localizing the Timezone\n",
    "\n",
    "Because MongoDB stores all timestamps in `UTC`, we need to figure out a way to localize it. Having timestamps in UTC might be useful if we were trying to predict some kind of global trend, but since we're only interested in what's happening with the air in Lagos, we need to change the data from UTC to `Africa/Lagos`. Happily, pandas has a pair of tools to help us out: [`tz_localize`](https://pandas.pydata.org/docs/reference/api/pandas.Series.tz_localize.html) and [`tz_convert`](https://pandas.pydata.org/docs/reference/api/pandas.Series.tz_convert.html). We use those methods to transform our data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04d148-e8e3-4d9a-a9fc-10e5316400b8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df.index = df.index.tz_localize(\"UTC\").tz_convert(\"Africa/Lagos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec7e24-1d2a-4c5e-9a2a-79a4b2cebf07",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Resampling Data\n",
    "\n",
    "The most important kind of data in our time-series model is the data that deals with time. Our `\"timestamp\"` data tells us when each reading was taken, but in order to create a good predictive model, we need the readings to happen at regular intervals. Our data doesn't do that, so we need to figure out a way to change it so that it does. The [`resample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html) method does that for us. \n",
    "\n",
    "Let's resample our data to create 1-hour reading intervals by aggregating using the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a11928-52f4-448d-beef-0c78bd92e92e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# `\"1H\"` represents our one-hour window\n",
    "df = df[\"P2\"].resample(\"1H\").mean().fillna(method=\"ffill\").to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019220d8-406d-452c-ac64-938d6db60884",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Notice the second half of the code:\n",
    "\n",
    "```python\n",
    "fillna(method=\"ffill\").to_frame()\n",
    "```\n",
    "\n",
    "That tells the model to **forward-fill** any empty cells with **imputed** data. Forward-filling means that the model should start imputing data based on the closest cell that actually has data in it. This helps to keep the imputed data in line with the rest of the dataset. \n",
    "\n",
    "## Adding a Lag\n",
    "\n",
    "We've spent some time elsewhere thinking about how two sets of data &mdash; apartment price and location, for example &mdash; compare to *each other*, but we haven't had any reason to consider how a dataset might compare to *itself*. If we're predicting the future, we want to know how good our prediction will be, so it might be useful to build some of that accountability into our model. To do that, we need to add a **lag**.\n",
    "\n",
    "Lagging data means that we're adding a delay. In this case, we're going to allow the model to test itself out by comparing its predictions with what actually happened an hour before. If the prediction and the reality are close, then it's a good model; if they aren't, then the model isn't a very good one.\n",
    "\n",
    "So, let's add a one-hour lag to our dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ba14a-46fd-4763-ab8b-145d9c44e70d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# In `shift(1), the `1` is the lagged interval.\n",
    "df[\"P2.L1\"] = df[\"P2\"].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74639-fe8a-47a3-ac2c-49c9cef99715",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Finally, let's drop our null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa58143a-83d9-4225-a091-29c583ea0f11",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "y = df[\"P2\"].resample(\"1H\").mean().fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77dd309-0abe-4213-a329-d05852d1cf15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Clean the Site 2 data from `lagos`, and save it as a Series called `y_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf4ae9-fb22-4659-895b-e27be5eb714a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df_prac.index = ...\n",
    "df_prac = ...\n",
    "df_prac[\"P2.L1\"] = ...\n",
    "\n",
    "\n",
    "y_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96cc3e-8f61-4deb-ab7e-7251ec7c8aed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5c73e-26d7-4c4c-b0b4-aec48ab49a8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Creating an ACF Plot\n",
    "\n",
    "Let's make an ACF plot using our `y` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f174d5-c693-4521-9984-608d3e15dc29",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots(figsize=(15, 6))\n",
    "# This is where to include your Series\n",
    "\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c54acc-93e1-4c33-adc5-789d4141b954",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Each of the dots on our plot represents a correlation coefficient. The first data point in the top left of the graph tells us that at time-step 0, the correlation coefficient was 1, meaning that there was a perfect correlation. That makes sense, because you can't lag from time-step 0, so the coefficient can't be anything other than 1. But, starting at hour 1, the coefficient drops precipitously, and we see our autocorrelation coefficients slowly decay over time. As our lag recedes further into the past, the correlations break down; a prediction you made five hours ago about what's happening right now is going to be a lot more reliable than a prediction you made 96 hours ago.\n",
    "\n",
    "The light blue shape across the bottom of the graph represents the **confidence interval**, or the extent to which we can be sure that our estimated correlations reflect the correlations that exist in reality. By default, this is set to 95%. Data points which fall either above or below the shape are likely not due to chance, and those which fall inside the shape are likely due to chance. It looks like all our data is the result of some kind of effect, so we're good to go.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make an ACF plot called `fig2` using your `y_prac` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e104d-29af-4b09-84f8-347371ca9ebd",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig2, ax = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735a570-a592-48c8-ad48-be4a66e0f68f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Creating a PACF Plot\n",
    "\n",
    "Let's make a PACF plot using our `y` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fd648-c207-4d6b-a7f8-6857845ec1ef",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f134b-593c-4f92-b9e3-321cbf5dc5b2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Aha! This looks very different. There are two things to notice here:\n",
    "\n",
    "First, we now have lots of data points that we can be relatively certain aren't due to chance, but we also have lots of data points inside the blue shape at the bottom, indicating that some of our data points are indeed due to chance. That's not necessarily a problem, but it's something useful to keep in mind.\n",
    "\n",
    "Second, recognize that even though the amplitude of the points on our graph has been significantly reduced, the trend has remained essentially the same: Strong positive correlations at the beginning, with the effect decaying over time. We would expect to see that, because the farther out into the future our predictions go, the less accurate they become. \n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make an PACF plot using your `y_prac` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b819645-5e75-4e1b-93f6-2da7e6c956a2",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig2, ax = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf9f5d-9fb0-4bca-8ac9-c24903932354",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Making a Line Plot with Rolling Averages\n",
    "\n",
    "Since we're interested in making predictions about the air quality in Lagos, it would be helpful to understand the rolling average for the PM 2.5 readings with a line plot. To keep things manageable, we'll set our window-size to one week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb472e-a4b2-4605-8959-117da242abd7",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "# `168` is the number of hours in a week.\n",
    "df[\"P2\"].rolling(168).mean().plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8dbd9-810d-4d54-9c76-1307f68f720a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Even though there are lots of peaks and valleys here, we're starting to see an emerging trend.\n",
    "\n",
    "We can make the same graph using pandas, like this:\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make a line plot that shows the weekly rolling average of the `P2` values in the `Site 2` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016358cb-3508-423c-823b-e48c8150c561",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcc6e88-8590-41e9-87a9-ad9ed4e1d4fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Splitting the Data in pandas\n",
    "\n",
    "The last thing to do in our data exploration is to split our data into training and test sets. For linear regression, we used an 80/20 split, where we used 80% of the data was our training set, and 20% of it was our test set. This time, we're going to expand the test set to 95%, and decrease the test set to %5 to bring it into line with `statsmodels` default confidence interval. This is important, because we'll need to use as much training data as we can if our model is going to accurately predict what's going to come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9841f7-7cde-430d-9629-fa1ee94b0024",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "cutoff_test1 = int(len(y) * 0.95)\n",
    "\n",
    "y_train = y.iloc[:cutoff_test1]\n",
    "y_test = y.iloc[cutoff_test1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4552f1-753e-49a3-8d6e-42157bfa1c37",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a cutoff called `cutoff_test2`, split the `y_prac`Series into training and test sets, making sure to set the cutoff to 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97b3e6-2ac9-4ecf-b074-195ed04612f6",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "cutoff_test2 = ...\n",
    "\n",
    "y_prac_train = ...\n",
    "y_prac_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e379b2-459a-4242-b523-6fb95d9b4ed6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1841c-6554-403b-8956-609d3fd2c7a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Baseline\n",
    "First, let's calculate the baseline MAE for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b9a53-4c0f-4f44-9c35-bee2545eaf2a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_mean = y_train.mean()\n",
    "y_pred_baseline = [y_train_mean] * len(y_train)\n",
    "mae_baseline = mean_absolute_error(y_train, y_pred_baseline)\n",
    "\n",
    "print(\"Mean P2 Reading:\", round(y_train_mean, 2))\n",
    "print(\"Baseline MAE:\", round(mae_baseline, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650281bd-2c4f-4a2c-9dae-2128856c77fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Calculate the baseline mean and MAE for the `y_prac` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd24dc-3857-4ecc-a0e1-736a20067636",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_train_mean = ...\n",
    "y_prac_pred_baseline = ...\n",
    "mae_baseline_prac = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd19bb-5441-4480-9564-e050ee284605",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Iterating\n",
    "\n",
    "Before we can go any further, we need to instantiate an **autoregression model** based on our `y` training data. We'll call the model `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834de4b-33de-4907-8455-187b21274e20",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model = AutoReg(y_train, lags=24, old_names=False).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492ef77-ca63-44d5-a5c7-5a06ca517f14",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Notice that, unlike our linear regression model which we built using scikit-learn, we're combining instantiation and fitting into one step; statsmodels includes that ability in its `AutoReg` method.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create and fit an autoregression model called `model_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2306ddf-8fc0-49a6-9ab4-91217ff3d2af",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a782b1-78a9-4cb8-9ea9-07f1a3c15d68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Autoregression models need us to generate **in-sample predictions** in order to calculate the MAE of our training data. In-sample predictions use data that's already part of our sample. That's to distinguish it from out-of-sample predictions, which we'll talk about a little bit later. The statsmodels library includes a method called [`predict`](https://www.statsmodels.org/stable/examples/notebooks/generated/predict.html) that can help us here. Above, the `AutoReg` method includes this line:\n",
    "\n",
    "```python\n",
    "old_names=False\n",
    "```\n",
    "\n",
    "The `False` value here tells the model that it can use in-sample lagged values to make predictions; if the value had been `True`, the model would have to look elsewhere to make its predictions.\n",
    "\n",
    "Here's how to generate in-sample predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ef5b2-b174-40c5-980e-4b5fc531f420",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74517d7a-1107-43c5-8b32-f2e998c72ae3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Once we've done that, we can calculate the MAE of the predictions in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a45dc7-9d76-4c7b-a8c3-7f9ce3e56341",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "training_mae = mean_absolute_error(y_train.loc[y_pred.index], y_pred)\n",
    "print(\"Training MAE:\", training_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5576a0b-1c67-4676-b845-3b793208a56d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Generate in-sample predictions using `y_prac`, and find the MAE for your `y_prac` training data. Print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353abd51-2c1b-4b7b-a79a-7254895c52d9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_pred = ...\n",
    "training_mae_prac = mean_absolute_error(\n",
    "    y_prac_train.loc[y_prac_pred.index], y_prac_pred\n",
    ")  # REMOVERHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee7cbd-374d-486b-a9b9-f403782a897f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Residuals\n",
    "\n",
    "We're going to use our model's residuals to make some visualizations, but first, we need to calculate what those residuals are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40112cb-3de8-45d8-a5cc-5a877cccaa51",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_resid = y_train - y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b73da7-9aa7-4e90-be6c-fa1261979559",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we can make a line plot of our model's residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25be9a-6eeb-4a4d-b989-b2302f59352a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots(figsize=(15, 6))\n",
    "y_train_resid.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec1560-7153-4f88-810f-3798c25085c6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The ideal residual plot has a random set of datapoints spread evenly on both sides of the line. The plot we just made actually looks pretty good; there are some significant outliers, but, on the whole, the bars describe an even band of values, which is what we're looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01a41b-b38c-490f-a30a-22b884ac6735",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Calculate the residuals for `y_prac` and visualize them on a line plot called `fig2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe766c4c-8a19-4f0e-bc1a-9bd5f4d2be76",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_train_resid = ...\n",
    "fig2, ax = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace2935-2a92-4c58-baab-838c387d08d4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's also take a look at a histogram of the residuals to help us see how they're distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73dc0b-79ed-4f86-bbe4-cba01e4d8e3d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_resid.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c74dc-f8a9-4bc7-92c8-89ea4a660f0d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Remember, when we make histograms, we're trying to answer two questions: \n",
    "\n",
    "1.) Is it a normal distribution?\n",
    "2.) Are there any outliers?\n",
    "\n",
    "For our histogram, that middle bar is pretty tall, but the shape described by all the bars looks like a normal distribution (albeit a stretched one), so the answer to the first question is \"yes.\" Outliers are values that fall beyond the shape of a normal distribution, and it doesn't look like we have any of those, so the answer to the second question is \"no.\" Those are the answers we're looking for, so let's move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc893f-77cd-45e4-a298-542acf86a037",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### ACF Plots\n",
    "\n",
    "We're going to make an ACF plot to see how much variation there is in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a2534-69aa-465f-b467-92d25e8f7658",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plot_acf(y_train_resid.dropna(), ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1a078d-a53c-427f-81ac-378f61205fbd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "At first, this might seem wrong, but we're actually looking for a mostly-flat graph here. This is an indication that our model describes all the **seasonality**, or regular changes, in our data. In other words, this graph is exactly what we're looking for.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Calculate the make a histogram and an ACF plot of the `y_prac` data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438cefc-7f92-4a23-a413-2db4914afb31",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad2b01-0555-43db-bef8-b57cdca6c647",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e6aac61-2ddc-4290-8a4f-4c63262cb15f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Now that we've built an autoregression model that seems to be working pretty well, it's time to **evaluate** it. We've already established that the model works well when compared to itself, but what about how well it works when we start looking outside our original dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5140a44-1a79-48a6-bd50-3689ca7a8844",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Out-of-Sample Predictions\n",
    "\n",
    "To look outside the data, we need to create a new set of predictions. The process here is very similar to the way we made out **baseline** predictions. We're still using [`predict`](https://www.statsmodels.org/0.6.1/examples/notebooks/generated/predict.html), but we're using the `test` data instead of the `train` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eee6fc-e7a9-4dbe-aaa9-4796f2f0001c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(y_test.index.min(), y_test.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f0235-b736-4698-8955-4422da9e1174",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we have a prediction, we can calculate the MAE of our out-of-sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd4fec-efc8-4623-bc5a-f1aa13a0dbf8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"Test MAE 1:\", test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e4e3f-3307-4808-b713-8e100cc3d8a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Generate out-of-sample predictions using your `y_prac` data and `model_prac`, calculate the MAE, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ffb661-6b1e-480c-95c6-15652ab3141c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_pred_test = model_prac.predict(\n",
    "    y_prac_test.index.min(), y_prac_test.index.max()\n",
    ")  # REMOVERHS\n",
    "test_mae_prac = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b8196-2997-4ba4-9b7d-c6a13e3f344e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we have some out-of-sample predictions, we can compare it to our in-sample predictions using a line plot. The first step there is to create a new DataFrame called `test1_predictions` with two columns: one for the `y_test` data (the true data) and one for the `y_pred` (the predicted data). It's always a good idea to print the first five rows of a new DataFrame, just to make sure it looks the way it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c9f63-2367-4358-b31b-e3022ad9cab5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test1_predictions = pd.DataFrame(\n",
    "    {\"y_test\": y_test, \"y_pred\": y_pred_test}, index=y_test.index\n",
    ")\n",
    "test1_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc39aa-ee9e-49dc-bffb-85befab48410",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "That looks correct, so we can move on to our line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59873f94-a40b-4967-9b5f-cc2df83712ed",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig = px.line(test1_predictions, labels={\"value\": \"P2\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43a10d-4294-4eda-b18a-2107849fa01b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This looks kind of strange, but it's actually exactly what we would expect to see. At the beginning, the `y_pred` data has a fair amount of predictive power, but, as time goes on, the predictions become less and less accurate. It's kind of like what happened with our ACF plots, only in reverse. Last time, the model lost its predictive power as the lag increased. Here, the model loses its predictive power as the horizon &mdash; how far away from the present your predictions are &mdash; increases.  But don't worry! We'll fix it in a second.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "In the meantime, try it yourself! Make a DataFrame with columns for `y_prac_test` and `y_prac_pred`, and print the result. Then, make a line plot that shows the relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da95516-5715-472c-b1a7-e00ce977c905",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1473d-3599-426c-8ff8-1328273653aa",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6057f23-f543-46b4-9a9a-d27b79f4be16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Walk-forward Validation\n",
    "\n",
    "Our predictions lose power over time because the model gets farther and farther away from its beginning. But what if we could move that beginning forward with the model? That's what **walk-forward validation** is. In a walk-forward validation, we re-train the model at for each new observation in the dataset, dropping the data that's the farthest in the past. Let's say that our prediction for what's going to happen at 12:00 is based on what happened at 11:00, 10:00, and 9:00. When we move forward an hour to predict what's going to happen at 1:00, we only use data from 10:00, 11:00, and 12:00, dropping the data from 9:00 because it's now too far in the past. Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156175fb-452d-45ae-b3ae-9145cc2a664d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# First, we define a walk-forward variable\n",
    "y_pred_wfv = pd.Series()\n",
    "# Then, we define a variable that takes into account what's happened in the past\n",
    "history = y_train.copy()\n",
    "# The `for` loop tells the model what to do with those variables.\n",
    "for i in range(len(y_test)):\n",
    "    # Here's where we generate the actual AR model\n",
    "    r = AutoReg(history, 24, old_names=False).fit()\n",
    "    # Now we're using `forecast` to create our next prediction\n",
    "    next_pred = r.forecast()\n",
    "    # We're adding the next prediction to the list\n",
    "    y_pred_wfv = y_pred_wfv.append(next_pred)\n",
    "    # And finally updating `history` to take into account the new observation\n",
    "    history = history.append(y_test[next_pred.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d0d37-4bd5-4366-9e27-c2cbea47ba2c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You'll notice that we're using the same `AutoReg` method we used before, only this time, we're using the `y_train` data. Also like before, the `24` is telling the model how many hours it should pay attention to. If you change that number, the MAE will change too.\n",
    "\n",
    "Speaking of the MAE, let's calculate it and see what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68baf11-4e5b-4ec1-bdec-c455be2b2e8b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test1_mae = mean_absolute_error(y_test, y_pred_wfv)\n",
    "print(\"Test MAE 1 (walk forward validation):\", round(test1_mae, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6810ede-f4d3-49eb-afc6-b5214ea806b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Perform a walk-forward validation of your model using the `y_prac_train` data. Then, calculate the MAE and print the result. Note that because we're using `%%capture` in the validation cell, you'll need to create a new cell for your MAE calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6aabed-166a-4bc4-a231-926750b5e50e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "y_prac_pred_wfv = ...\n",
    "history_prac = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a364478-628d-4eb4-802e-81ddd01b7dce",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test2_mae = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8dd3c-0960-4a65-abf1-45e8504fec35",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Communicating the Results\n",
    "\n",
    "In machine learning, the model's **parameters** are the parts of the model that are **learned** from the training data. There are also **hyperparameters**, which we'll discuss in the next module. For now, just know that parameters come from inside the model, and hyperparameters are specified outside the model.\n",
    "\n",
    "So, let's print the parameters of our validated model and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e58cfb-3d93-4f64-8128-ff589ef80099",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45968a-eee1-4a7d-a620-e2abbe638b80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "That looks pretty good, but showing it in a line plot would be much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51347ee2-ebf5-4700-ab04-2a230dcee19c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test1_predictions = pd.DataFrame(\n",
    "    {\"y_test\": y_test, \"y_pred\": y_pred_wfv}, index=y_test.index\n",
    ")\n",
    "fig = px.line(test1_predictions)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d6329-6b85-4291-8d61-97225f32da1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "That looks much better! Now our predictions are actually tracking the `test` data, just like they did in the linear regression model.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Access the parameters of `model_prac`, put `y_prac_test` and `y_prac_pred_wfv` into the `test2_predictions` DataFrame, and create a line plot using plotly express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03718244-afbc-4fbc-b5b4-d8039664ddbb",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b46051-9b15-4ea0-9ab9-f061e5419188",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12bdc0b-93f3-42eb-884e-353df89bd2e4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# ARMA Models & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59166a77-d626-4116-bc20-5edfdc00e1d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**ARMA** stands for Auto Regressive Moving Average, and it's a special kind of **time-series** analysis. So far, we've used autoregression (AR) to build our time-series models, and you might recall that AR models rely on values that remain relatively stable over time. That is, they can predict the future very well, as long as the future looks roughly the same as the past. The trouble with predicting the future is that things can suddenly change, and as a result, the future doesn't look much like the past anymore. These sudden changes &mdash; economists call them *endogenous shocks* &mdash; can be as big as a hurricane destroying a city or an unexpected increase in the minimum wage, and they can be as small as a new restaurant opening in a neighborhood or a single person losing their job. In our data, the air quality might be changed if there was a nearby forest fire, or if a building collapsed near one of the sensors and raised a giant cloud of dust. \n",
    "\n",
    "Regardless of the size of the shock, ARMA models can *still* predict the future. All we need to make that work is data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ada381-6590-431d-805c-e4eab38903c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "As always, we need to import all the tools we'll need to make our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927ff0e-2c3f-40ab-90ea-3bd1a5469aa0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab299f8-1443-49d7-b91c-2f6a627fe739",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And then we need to get our database client up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353dd333-9284-4562-b709-51792117994f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient(host=\"localhost\", port=27017)\n",
    "db = client[\"air-quality\"]\n",
    "lagos = db[\"lagos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e34a36-c577-4da7-a25c-4de7282e5550",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Then, we need to clean our data. All the examples will use data from Site 3; all the practice sets will use Site . If you need a refresher on how all those methods work, refer back to the Autoregression notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25412a-7b45-426f-9654-79fc1c4edc00",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "results = lagos.find(\n",
    "    # Note that the `3` refers to Site 3.\n",
    "    {\"metadata.site\": 3, \"metadata.measurement\": \"P2\"},\n",
    "    projection={\"P2\": 1, \"timestamp\": 1, \"_id\": 0},\n",
    ")\n",
    "df = pd.DataFrame(list(results)).set_index(\"timestamp\")\n",
    "df.index = df.index.tz_localize(\"UTC\").tz_convert(\"Africa/Lagos\")\n",
    "df = df[df[\"P2\"] < 500]\n",
    "df[\"P2.L1\"] = df[\"P2\"].shift(1)\n",
    "df.dropna(inplace=True)\n",
    "y = df[\"P2\"].resample(\"1H\").mean().fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d7ee4-1177-4780-84f7-748a52edcc3a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Get your client up and running and call your database `db_prac`. Create a variable called `results_prac`, and read in a collection called `lagos_prac` using data from Site 2. Save it as a Series called `y_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9101327-d4b1-436f-8bdd-d4e33540251a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "db_prac = ...\n",
    "lagos_prac = ...\n",
    "\n",
    "df_prac = ...\n",
    "df_prac.index = ...\n",
    "df_prac = ...\n",
    "df_prac[\"P2.L1\"] = ...\n",
    "\n",
    "y_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8d7f3-7e1a-45d5-a485-028d6cbc10e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Exploring the Data\n",
    "\n",
    "Just like we did with AR, we'll start by exploring the data. Let's make a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a653f-8739-46f3-a4ae-4f99ef3a6c0d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759698d-4541-48ac-ba69-c7c346810af1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make a histogram using `y_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8bb30b-f170-4d1b-8306-086c04178007",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdf637f6-fcc6-49f7-88a3-00b5e3805a8f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This is what the data looks like when our sample is 1-hour intervals, but we might want to be able to quickly change our sample to other intervals of time. First, we'll create a function called `wrangle`, and then add an **argument**. In Python, arguments tell the function what to do. This function already has an argument called `collection`, so we'll need to add another to make resampling work. We'll call that argument `resamp_pd`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5eea4-c9a1-402a-829f-831f5a6ef6eb",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here's where the new argument goes. We're setting the default value to `\"1H\"`.\n",
    "def wrangle(lagos, resamp_pd=\"1H\"):\n",
    "    results = lagos.find(\n",
    "        # Note that the `3` refers to Site 3.\n",
    "        {\"metadata.site\": 3, \"metadata.measurement\": \"P2\"},\n",
    "        projection={\"P2\": 1, \"timestamp\": 1, \"_id\": 0},\n",
    "    )\n",
    "    df = pd.DataFrame(list(results)).set_index(\"timestamp\")\n",
    "    df.index = df.index.tz_localize(\"UTC\").tz_convert(\"Africa/Lagos\")\n",
    "    df[\"P2.L1\"] = df[\"P2\"].shift(1)\n",
    "    df.dropna(inplace=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7afae9-51fc-4e6c-bf13-1e012ee86aea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now let's change `\"1H\"` to `\"1D\"` and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cb048-127d-4d31-a1ad-a3f0d0e879f0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y = wrangle(lagos, resamp_pd=\"1D\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a1e02-da94-48d0-b595-0ff1dd9e1798",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As you can see on the left side of the table, the samples are now at one day intervals, which is exactly what we wanted!\n",
    "\n",
    "Let's make a new histogram to see if changing the sampling interval made a difference in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b1b22-6d1c-41ca-99bc-51c4788dac4d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f00a1-f1e9-44d9-85df-877c23e94735",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This looks pretty different! It's always nice to have a diversified dataset.\n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Define a function called `wrangle_prac` run it, and print the results of `y_prac`. Then, create a new histogram from `y_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e5fc1-2922-4fc9-b631-0b33099860f2",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(y_prac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefe49c-78e2-418a-af8f-448e54c2e23d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "110e580b-a712-4825-86d2-867558cd7a65",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Like with our AR model, we need to create ACF and PACF plots to see what's happening with the correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b891e1-1400-4ef0-9a67-098120269736",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "plot_acf(y, ax=ax)\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4810ca-660a-4fee-825a-4487172e6b40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And now let's make a PACF plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd30853-5ae9-4bc4-9f2a-bb8d8e830eb7",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "fig = plot_pacf(y, ax=ax)\n",
    "plt.xlabel(\"Lag [hours]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b6bad-4b69-48bb-ad54-c0ea49de4430",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Make a PAC and a PACF plot using your `y_prac` data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78e971-57a2-41ce-b887-c7abfb8b9b48",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190be9b-7173-434c-acb1-5e5f0023f130",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25b45224-ec64-4b42-984c-68f99388c531",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Splitting the Data\n",
    "\n",
    "In our AR model, we split our data based on the number of observations we wanted to investigate. This time, we're going to split our data based on the date, using just the readings from October of 2018. So, just like we did before, we'll create a training set using `y`, but instead of using percentages to split the data, we'll use dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303daec-2e66-4bc3-95fa-d74fa78dd425",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Notice that the date format is `YYYY-MM-DD`\n",
    "y_train = y.loc[\"2018-10-01\":\"2018-10-31\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea2100-5be5-49c3-b9a0-b1280a05e2eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a training dataset called `y_prac_train` based on November of 2017. (Hint: there are 30 days in November.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868ff74-0bab-4664-8269-6484c91daa83",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_train = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc82f22-d8fb-473d-be31-0f3ca935a9ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6376f-12a3-4ec0-984a-1a8943e272c3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Baseline\n",
    "\n",
    "The first thing we need to do is calculate the MAE for our new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bf9b2-05e3-401c-bfc0-08781a402c65",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_mean = y_train.mean()\n",
    "y_pred_baseline = [y_train_mean] * len(y_train)\n",
    "mae_baseline = mean_absolute_error(y_train, y_pred_baseline)\n",
    "print(\"Mean P2 Reading:\", round(y_train_mean, 2))\n",
    "print(\"Baseline MAE:\", round(mae_baseline, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271876c6-222c-4e99-9d08-35adb9ce0fe7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Calculate the mean and MAE for the `y_prac` Series, and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c6220-b23d-4c57-9be8-0f6afb48bb85",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_prac_train_mean = ...\n",
    "y_prac_pred_baseline = ...\n",
    "mae_baseline_prac = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7164ecf-d67b-4323-9f5b-1f4f691d89e8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Iterating\n",
    "\n",
    "So far, the only difference between our old AR model and the new ARMA model we're building is that the new model's data is based on the date rather than on the length of the variable. But the difference between AR and ARMA is the addition of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceedc11-63e1-429a-b153-58f5092c0cc3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Let's set our `p` values to include values from 0 to 25, moving in steps of 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d940da-21f9-443d-8814-042889639116",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "p_params = range(0, 25, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345f7ce-5939-4162-a995-250e585f6a67",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And let's set our `q` values to include values from 0 to 3, moving in steps of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879cb13-b92c-4409-8e6a-579568716bc0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "q_params = range(0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffaddaf-a3e9-4e6a-99b3-94cb117acf30",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Using `p_params_prac`, set the `p` value to include vales from 1 to 4, moving in steps of 1. Then, using `q_params_prac`, set the `q` value to include values from 0 to 3, moving in steps of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe13001-e730-4677-8061-7875d8e2f31b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "p_params_prac = ...\n",
    "q_params_prac = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bbd2f-1553-46bb-9ec6-42142cbb18dd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In order to tell the model to keep going through all the possible combinations, we'll add in a pair of `for` loops. (If you need a refresher on `for` loops, refer to Notebook 001.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b37a6-eb5e-4a09-808a-4b6ecb321886",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "maes = dict()\n",
    "for p in p_params:\n",
    "    maes[p] = list()\n",
    "    for q in q_params:\n",
    "        order = (p, 0, q)\n",
    "        start_time = time.time()\n",
    "        # Here's where we actually define the model\n",
    "        model = ARIMA(y_train, order=order).fit()\n",
    "        # Here's where we tell the model how we want it to deal with time\n",
    "        elapsed_time = round(time.time() - start_time, 2)\n",
    "        print(f\"Trained ARIMA {order} in {elapsed_time} seconds\")\n",
    "        # Here's where we get back into the MAE for the model\n",
    "        y_pred = model.predict()\n",
    "        mae = mean_absolute_error(y_train.iloc[24:], y_pred.iloc[24:])\n",
    "        # And finally we append the MAES to the original list\n",
    "        maes[p].append(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccee37-7a04-408c-a31b-0e54916a5642",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create an ARMA model called `mode_prac2` based on a dictionary called `maes_prac`, using your training and test data, then print the results and append the MAE to `maes_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9eac7b-f22e-4fc4-a32e-9238fdeba9f4",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69d3aba4-a18f-436c-9958-3e2847a365e9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we have a working ARMA model, let's turn the output into a DataFrame so we can see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e677fa-4bbe-4ecd-b49f-10efa33a823c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mae_grid = pd.DataFrame(maes)\n",
    "mae_grid.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e47e02-26e2-4cf9-be77-db977b61dfab",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And let's visualize the DataFrame in a heatmap. (If you need a refresher on how to create a heatmap in seaborn, refer to Notebook 008.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c6b62-af13-4ae0-9200-ade359605813",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(mae_grid, cmap=\"Blues\")\n",
    "plt.xlabel(\"p values\")\n",
    "plt.ylabel(\"q values\")\n",
    "plt.title(\"Grid Search (Criterion: MAE)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb54cf-ec6e-45b1-942c-5fe040518732",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Turn read the output of your ARMA model into a DataFrame called `mae_grid_prac`, and visualize it in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f48084-10ee-4882-ac68-638f6cfda199",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mae_grid_prac = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bcf007-0b33-4b49-980d-1a7087590d01",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "It looks like our MAE values are in the right place, but lets try some other ways to explore our new model using the `plot_diagnostics` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cec8f-44c0-4287-8fe8-811690846240",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "model.plot_diagnostics(fig=fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d1732-846d-4c99-b215-424db98368b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As usual, we have quite a lot to sift through here. The first graph is showing us our model's residuals. Ideally, we'd like to see this be as close to zero as possible, and this graph is telling us that, for the most part, we have a good model.\n",
    "\n",
    "The next graph over shows us another version of the same thing. The histogram is similar to the one we made before, but there's a pair of lines superimposed. These lines are indicating the **kernel density**, which is another way of saying that it's a smoothed-out version of the blue histogram bars. The green line represents a normal distribution, and is included here just to give you something to compare to the values from your model. The orange line represents the smoothed-out version of the result off your model. Our model is actually pretty close to a normal distribution, so that's good!\n",
    "\n",
    "The Q-Q plot on the bottom left is yet another way to visualize the same thing. Here, the red line is showing us a perfect 1:1 correlation between our variables, and the wavy blue line is showing us what we actually have. Again, our model is pretty close to the red line, so it's looking good.\n",
    "\n",
    "And finally, we have a correlogram, which might look familiar; it's the same kind of plot as the ACF and PACF plots from our AR models. \n",
    "\n",
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Use `plot_diagnostics` to examine the residuals from `model_prac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e0901-5ce7-4b16-9836-1d5c0ddd02af",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21e52359-095b-4226-bcb7-da25290c3476",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Communicating the Results\n",
    "\n",
    "Now that we have an ARMA model that seems to be working well, it's time to communicate the results of our analysis in a line graph. Let's create a graph that shows the relationship between our training and predicting data. (For a refresher on how to do this and what it means, refer to the AR notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0f2ee-a3a8-4b07-a93b-6f4c729ffa70",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y_train_pred = model.predict()\n",
    "df_predictions = pd.DataFrame(\n",
    "    {\"y_train\": y_train, \"y_pred\": y_train_pred}, index=y_train.index\n",
    ")\n",
    "fig = px.line(df_predictions, labels={\"value\": \"P2\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5931c2-5a9d-4fc1-9ad3-73d7b48d73eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a line plot that compares the `y_prac_train` and `y_prac_pred` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb43227-e5ce-43a0-b414-6b6418a8d63e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75b64806-b6ea-44d9-9bb8-3072275591ce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# References & Further Reading\n",
    "\n",
    "- [More on ARMA models](https://365datascience.com/tutorials/time-series-analysis-tutorials/arma-model/)\n",
    "- [Even more in ARMA models](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)\n",
    "- [Information on p and q parameters](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)\n",
    "- [A primer on autoregression](https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/)\n",
    "- [More information on ACF plots](https://www.statisticshowto.com/correlogram/#:~:text=A%20correlogram%20(also%20called%20Auto,a%20subsequent%20point%20in%20time.)\n",
    "- [A primer on ACF and PACF](https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/)\n",
    "- [Background on residuals](https://www.statisticshowto.com/residual/)\n",
    "- [More on walk-forward validation](https://www.tutorialspoint.com/time_series/time_series_walk_forward_validation.htm)\n",
    "- [Reading on parameters and hyperparameters](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df382cd-4890-4058-8176-0e382e099fc9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
