{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6039b09d-eead-48a7-a89e-cf89167f7e87",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+3\"><strong>Machine Learning: Data Pre-Processing and Production</strong></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1109dee-40b9-490f-89f4-141af683c320",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a209f57-35bf-4f14-96ff-b08908e99406",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# What's scikit-learn?\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/) is a Python library that contains implementations of many common machine learning algorithms and uses common interfaces for these that enables experimentation.  In this section, we'll look at **linear regression** (which you'll use to predict price based on the area of a property) and **K-nearest neighbors**, which you'll use to classify the neighborhood a property is in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576265a-2fd2-417f-b411-947910741faa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64512bad-51af-463a-a263-70917f27966f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a3fd1-fd14-47ea-956c-56ba35a3e370",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Standardization** is a widely used scaling technique to transform features before fitting into models. Feature scaling changes all a dataset's continuous features to give us a more consistent range of values. Specifically, we subtract the mean from each data point and then divide by the standard deviation:\n",
    "\n",
    "$$ \\hat{X} = \\frac{X-\\mu}{\\sigma}, $$\n",
    "\n",
    "The goal of standardization is to improve model performance having all continuous features be on the same scale. It's useful in at least two circumstances:\n",
    "\n",
    "1. For machine leaning algorithms that use Euclidean distance (k-means and k-nearest neighbors), different scales can distort the calculation of distance and hurt model performance.\n",
    "1. For dimensionality reduction (principal component analysis), it can improve the model's ability to finds combinations of features that have the most variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488097c-ca07-44ca-a7c9-6e51d1c35b6d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's check the following example where we apply standardization on one of the columns in the following DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b784bb5-fc40-48fc-81b3-4418b9b01371",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV into DataFrame\n",
    "df = pd.read_csv(\"./data/mexico-city-test-features.csv\").dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc6782-f7a8-4c03-9df5-c022b8646354",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Our target feature is the `\"surface_covered_in_m2\"` column. Let's first check the maximum and minimum of this column before standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619599e-2111-449b-ac46-1ffd047b2cd0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print(\"Maximum before standardization is:\", df[\"surface_covered_in_m2\"].max())\n",
    "print(\"Minimum before standardization is:\", df[\"surface_covered_in_m2\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e31594-3d85-4fe6-8106-7c21dde49d7d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can perform the transformation by first instantiating the scaler and assigning the feature to a variable name. Then we fit the scaler and transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16635ed3-af07-444d-83fb-acea5c590815",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Name the scaler and targeted features\n",
    "scaler = StandardScaler()\n",
    "X_train = df[[\"surface_covered_in_m2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c76741-dd02-40e3-9eb1-6ede96a78ac3",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Fit the scaler to feature\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072890c-c3e9-47fd-8550-cb83732742ae",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Pass the scaler to feature to transform data\n",
    "X_transformed = scaler.transform(X_train)\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa97f4-3364-48cf-8b47-d64c0f7ffeb6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now you can see the transformed data range is much smaller after standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c746bf4-1468-43e9-9bc8-d022210573d9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print(\"Maximum after standardization is:\", X_transformed.max())\n",
    "print(\"Minimum after standardization is:\", X_transformed.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fe26e-fed7-4986-8e4d-392e52cd1d48",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can also combine the fit and transform process together into one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b990c-bf5f-41e8-9886-83c7f51fff57",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "X_transformed = scaler.fit_transform(X_train)\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c56570-7833-4570-b8eb-e1302ae228a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>  \n",
    "\n",
    "Standardize the price column in `\"mexico-city-real-estate-1.csv\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a5fb7-ebbb-4c97-9da8-f90b493108d5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/mexico-city-real-estate-1.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca18296-6b36-487a-b204-c84efa957be3",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "scaler = ...\n",
    "X_train = ...\n",
    "X_transformed = ...\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10a193-bdaa-426a-ae74-c45c2bb7f2b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7cb95f-58e2-4d6b-bc97-e090b39dace2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "A property's district is **categorical data**, or data which can be divided into groups.  For many machine learning algorithms, it's common to create a column in a DataFrame to indicate if the feature is present or absent, instead of using the category's name. First you a column for each district names then, for each observation, you put a 1 or a 0 to indicate if the property is located in each neighborhood or not. Let's take a look at the `mexico-city-test-features.csv` dataset for properties which include the district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430e8e6-6490-4109-9186-4c73c8affe31",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV into DataFrame\n",
    "df = pd.read_csv(\"./data/mexico-city-test-features.csv\").dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b7a6c-70e8-4ad2-a2d9-9796785ab241",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You can do one-hot encoding using pandas [`get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) function, but we'll use a the [Category Encoders](https://contrib.scikit-learn.org/category_encoders/) library since it allows us to integrate the one hot encoder as a transformer in a scikit-learn Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747f8ce-3b56-4e1a-92ae-2129de855072",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "# Instantiate transformer\n",
    "ohe = OneHotEncoder(use_cat_names=True)\n",
    "\n",
    "# Fit transformer to data\n",
    "ohe.fit(df)\n",
    "\n",
    "# Transform data\n",
    "df_ohe = ohe.transform(df)\n",
    "\n",
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dfc107-3834-4c8d-b081-ece0231ae3a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>  \n",
    "\n",
    "Create a DataFrame which one-hot encodes the `property_type` column in `mexico-city-real-estate-1.csv`.  The DataFrame you create should have extra columns for apartments, houses, and stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfdf97e-ba9c-4739-8fbf-560a97a71e2d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mexico_city1 = pd.read_csv(\n",
    "    \"./data/mexico-city-real-estate-1.csv\", usecols=[\"property_type\"]\n",
    ")\n",
    "ohe = ...\n",
    "mexico_city1_ohe = ...\n",
    "mexico_city1_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ea6c1-3f0a-43e1-b30f-e28da6dec0bd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306aa5b4-ee4c-4665-bbf0-f2deb6ae9e5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "For many machine learning algorithms, it's common to use one-hot encoding. This works well if there are a few categories, but as the number of features grows, the number of additional columns also grows. \n",
    "\n",
    "Having a large number of columns (and consequently a large number of features in your model) can lead to a number of issues often referred to as the **curse of dimensionality**. Two primary issues that can arise are computational complexity (operations performed on larger datasets may take longer) and overfitting (the model may not generalize to new data). In these scenarios, ordinal encoding is a popular choice for encoding the categorical variable. Instead of creating new columns, ordinal encoding simply replaces the categories in a categorical variable with integers.\n",
    "\n",
    "One potential risk of ordinal encoding is that some machine learning algorithms assume the integer values imply an ordering in the variables. This is important in logistic regression, where a relationship is defined between increases or decreases in the features and the target. Techniques like decision trees are okay to use ordinal encoding, because they generate splits. Rather than assuming any ordering between the numeric values, the splits will occur between the numeric values and effectively separate them. You can use the `OrdinalEncoder` transformer to perform ordinal encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff56dad-be71-472f-98c0-2936f1e068d4",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "# Instantiate transformer\n",
    "oe = OrdinalEncoder()\n",
    "\n",
    "# Fit transformer to data\n",
    "oe.fit(df)\n",
    "\n",
    "# Transform data\n",
    "X_train_oe = oe.transform(df)\n",
    "\n",
    "X_train_oe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1bb4c-1190-4937-aa11-e5442994220d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>  \n",
    "\n",
    "Create a DataFrame which ordinal encodes the `property_type` column in `mexico-city-real-estate-1.csv`.  The DataFrame you create should have integers replacing the values for apartments, houses, and stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18937a-5039-40fd-ac2a-4f2b10168983",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mexico_city1 = pd.read_csv(\n",
    "    \"./data/mexico-city-real-estate-1.csv\", usecols=[\"property_type\"]\n",
    ")\n",
    "\n",
    "oe = ...\n",
    "mexico_city1_oe = ...\n",
    "mexico_city1_oe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584460f-70f7-446c-872f-43b674782eb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3863674-1242-4f9d-a9c0-68fcda677790",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Let's take a look at `mexico-city-real-estate-1.csv` and impute some of the missing values. First, we'll load the dataset, limiting ourselves to the `\"surface_covered_in_m2\"` and `\"price_aprox_usd\"` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101f488-1b1b-4105-8e53-11f7ca6e3c38",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "columns = [\"surface_covered_in_m2\", \"price_aprox_usd\"]\n",
    "mexico_city1 = pd.read_csv(\"./data/mexico-city-real-estate-1.csv\", usecols=columns)\n",
    "mexico_city1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a2aec-1a54-4120-a43e-d688a204234c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When you need to build a model using features that contain missing values, one helpful tool is the scikit-learn transformer [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html). In order to use it, we need to start by instantiating the transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d5f08-f124-4c74-b75a-e90523fbd068",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8abe0-e221-4e06-bebb-70c350c4ba7c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next, we train the imputer using the data. At this step it will calculate the mean value for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c8271-7d88-4396-ba85-ca7859bbf3e2",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "imputer.fit(mexico_city1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ea83c-1b0c-4907-b1e9-791ec2970812",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Last, we transform the data using the imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41a61e-7880-4cb9-9bde-acb85bf91557",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mexico_city1_imputed = imputer.transform(mexico_city1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470666e-5567-43d5-b0b3-27daabb04eed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Since the imputer doesn't return a DataFrame, let's transform it into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4e955-ec34-42ed-9bc1-aee0f078587b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mexico_city1_imputed = pd.DataFrame(mexico_city1_imputed, columns=columns)\n",
    "mexico_city1_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2add65a-1981-49a7-9bb6-e572b0a58d2c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now there are no missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64138c1b-961d-4dc9-b31a-31225e2bb606",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Then we use the imputer to transform the data.\n",
    "\n",
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Read `mexico-city-real-estate-1.csv` into a DataFrame and impute the missing values for `\"surface_covered_in_m2\"` and `\"price_aprox_usd\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12448203-db0f-4109-a822-38cbfd4562e1",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "columns = [\"surface_covered_in_m2\", \"price_aprox_usd\"]\n",
    "mexico_city2 = ...\n",
    "\n",
    "# Instantiate transformer\n",
    "imputer = ...\n",
    "\n",
    "# Fit transformer to data\n",
    "\n",
    "\n",
    "# Transform data\n",
    "mexico_city2_imputed = ...\n",
    "\n",
    "# Create DataFrame\n",
    "mexico_city2_imputed = pd.DataFrame(mexico_city2_imputed, columns=columns)\n",
    "\n",
    "mexico_city2_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b099d4-5284-4a4a-ab89-f6ba7080a150",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c906f9-c537-4fe4-bcf3-dd9e797fee4b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's consider the `mexico-city-real-estate-1.csv` dataset and fit a regression model using `surface_covered_in_m2` and `price_aprox_local_currency` to estimate `price_aprox_usd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa1166-a078-4491-983e-65972c33bddc",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Import data\n",
    "columns = [\n",
    "    \"price\",\n",
    "    \"price_aprox_local_currency\",\n",
    "    \"price_aprox_usd\",\n",
    "    \"surface_total_in_m2\",\n",
    "    \"surface_covered_in_m2\",\n",
    "    \"price_per_m2\",\n",
    "]\n",
    "\n",
    "mexico_city1 = pd.read_csv(\"./data/mexico-city-real-estate-1.csv\", usecols=columns)\n",
    "\n",
    "# Drop rows with missing values\n",
    "mexico_city1.dropna(inplace=True)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(\n",
    "    mexico_city1[[\"surface_covered_in_m2\", \"price_aprox_local_currency\"]],\n",
    "    mexico_city1[\"price_aprox_usd\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dbcc7-6c91-4cf1-bb13-5ee08aee6039",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now let's calculate the mean absolute error in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd4e11-0114-4509-9598-c3740a2704f4",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "price_pred = lr.predict(\n",
    "    mexico_city1[[\"surface_covered_in_m2\", \"price_aprox_local_currency\"]]\n",
    ")\n",
    "mean_absolute_error(price_pred, mexico_city1[\"price_aprox_usd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0049ba-ec84-46c3-abdc-45ab6671b234",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When you see a mean absolute error that's so close to zero (especially when the mean apartment price is so much larger), chances are there is leakage in your model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93d69e-4580-480e-a66c-f1d4070ce12b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be356a6-4f4b-43d7-8af9-89d7b07b8d3e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When dealing with classification problems, we would ideally expect the training data to be evenly spread across different classes for better model performance. When the numbers of observations are uneven in different classes, we have imbalanced data. The class that represents the majority of observations is called the **majority class**, while the class with limited observation is called the **minority class**. Imbalanced data limits training data available for certain classes. In addition, when the one class takes the majority of the data, the model will keep predicting the majority class to achieve high accuracy result. Thus, prior to training a  model, it is essential to balance the data either through under-sampling the majority classes, or over-sampling the minority classes, or use other evaluation metrics like **recall** or **precision**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2136be2-be91-42e6-8657-1c35ffb8a0a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Under-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1326f9-9327-45f6-9b61-83566c46cafa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When data is imbalanced in different classes, one way we can balance it is reducing the number of observations in the majority class. This is called **under-sampling**. We can under-sample by randomly deleting some observations in the majority class. The open source [imbalanced-learn](https://imbalanced-learn.org/stable/) (imported as `imblearn`) is an open-source library that works with `scikit-learn` and provides tools when dealing with imbalanced classes. Here's an example of randomly deleting observations from the majority class using Poland bankruptcy data from 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e582b7-7a45-4307-a2d8-5c48193952e0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "with gzip.open(\"data/poland-bankruptcy-data-2008.json.gz\", \"r\") as f:\n",
    "    poland_data_gz = json.load(f)\n",
    "\n",
    "df = pd.DataFrame().from_dict(poland_data_gz[\"data\"])\n",
    "\n",
    "df[\"bankrupt\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34ed2a-493f-418c-9a9c-01c4a3859086",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The data is clearly imbalanced as there are many more observations in non-bankruptcy compared to bankruptcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14155e26-8f7e-43b9-bc1a-f1239e3706e8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X, y = RandomUnderSampler().fit_resample(df[[\"company_id\"]], df[[\"bankrupt\"]])\n",
    "y[\"bankrupt\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec3b5f-4f46-4103-99db-b4a5f3ab9818",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we have reduced the non-bankruptcy class to the same size as the bankruptcy class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9c57c-7ea7-4511-8974-cc9126521901",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08099798-ae9a-4bbc-88db-1838496bc294",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Over-sampling** is the opposite of under-sampling. Instead of reducing the majority class, over-sampling increases the number of observations in the minority class by randomly making copies of the existing observations. Here is an example of making random copies from the minority class using the Poland bankruptcy data and `imblearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b0947-d80d-415d-8969-faa336aaed2d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "X, y = RandomOverSampler().fit_resample(df[[\"company_id\"]], df[[\"bankrupt\"]])\n",
    "y[\"bankrupt\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff2d94-4d6d-4748-9459-ac7345a6d011",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we have increased the bankruptcy class to the size of the non-bankruptcy class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ff7cd-98cb-46ae-9902-16c5dc954da8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee83935-0a2d-4973-ac15-488e4aac6584",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that you've seen an example of imbalanced data and how to under-  or over-sample it prior to model training, let's get some practice with the Poland bankruptcy data from 2007."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157466b5-0a36-4c76-b1c0-1f3fcce04553",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"data/poland-bankruptcy-data-2007.json.gz\", \"r\") as f:\n",
    "    poland_data_gz_2007 = json.load(f)\n",
    "\n",
    "df_2007 = pd.DataFrame().from_dict(poland_data_gz_2007[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad2e3a-cf1b-44b7-82a7-5f422bf63c4b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "First, check whether this data is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92768355-c866-461c-a570-8aeac55f46c5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06fb0288-5870-4f43-9846-ca6bad1eac34",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next, do under-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b1656-fa2f-419e-92c3-46e4cba77acb",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "X, y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af750bef-dac2-4f0e-b87a-f71a411914be",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Finally, check whether the data is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9568f-6201-4efb-9970-a5c0756c2110",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba488f97-f17d-4e65-a1b0-9520e65fe2be",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Great work! Now try over-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d6524-8b1a-477f-8995-b39d3f1e2d2b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "X, y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffec285-75a5-4bdd-a12b-878d048975b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And check whether the data is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87332b3e-c5a2-41b8-b472-b82f7b1228c4",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "121a9474-72ee-4f57-91ae-a26c9b9438cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# scikit-learn in Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d369ff8-78dd-49d1-b7f5-02058219a308",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The previous examples have built models and made predictions one step at a time.  Many machine learning applications will require you to run the same steps many times, usually with new or updated data.  scikit-learn allows you to define a set of steps to process data for machine learning in a reproducible manner using a pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ca383-f4cc-4080-b77b-4415b4b042e8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Creating a Pipeline in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719cca65-3413-4304-9a28-af8e60fea2a9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "First, we create a pipeline to do linear regression on the transformed data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8fd8f-d2ab-44d5-9a8c-acd6bdc74b6f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# construct pipeline\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "\n",
    "pipe = Pipeline([(\"regressor\", lin_reg)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7866b5-94ad-4ead-b277-a1e93e2cbab1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can check the steps in the pipeline, but right now, there's only 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9b85d-1f32-4d2d-bd90-8a1220c2ea49",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d04adaa-c3fb-452e-8848-678219de6c56",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Then we fit a linear regression model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6551911-7297-4356-9545-809f8cb6fdba",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# fit/train model and predict labels\n",
    "mexico_city1 = pd.read_csv(\"./data/mexico-city-real-estate-1.csv\")\n",
    "mexico_city1 = mexico_city1.drop(\n",
    "    [\n",
    "        \"floor\",\n",
    "        \"price_usd_per_m2\",\n",
    "        \"expenses\",\n",
    "        \"rooms\",\n",
    "        \"price_per_m2\",\n",
    "        \"price\",\n",
    "        \"surface_total_in_m2\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "mexico_city1 = mexico_city1.dropna(axis=0)\n",
    "mexico_city1[\"surface_covered_in_m2\"] = mexico_city1[\"surface_covered_in_m2\"].astype(\n",
    "    float\n",
    ")\n",
    "\n",
    "y = mexico_city1[\"price_aprox_usd\"]\n",
    "X = mexico_city1.surface_covered_in_m2.values.reshape(-1, 1)\n",
    "pipe.fit(X, y)\n",
    "y_pred = pd.DataFrame(pipe.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50d6f0-88b7-4c03-8a90-94b325a47d5b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print(y_pred.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d203a62-ca39-4842-af94-ad5a77fb632b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Try this on the  `price_aprox_usd` column in the `mexico-city-real-estate-1.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68739c85-bd36-4d97-9f10-6ee3ffa57130",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "y = ...\n",
    "X = ...\n",
    "pip.fit(...,...)\n",
    "y_pred = ...\n",
    "print(y_pred.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9ee5c-9667-47a5-92b6-7da970d5b69d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's use the `make_pipeline` function to create a pipeline to fit a linear regression model for the `mexico-city-real-estate-1.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4c3d5-9571-45bb-96b7-698e9e45b97a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "y = mexico_city1[\"price_aprox_usd\"]\n",
    "X = mexico_city1.surface_covered_in_m2.values.reshape(-1, 1)\n",
    "model_lr = make_pipeline(linear_model.LinearRegression())\n",
    "model_lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2ac7f-9b97-4118-9a7d-c385eccfa48d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's try to predict `price_aprox_usd` in the `mexico-city-test-features.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291c4c9-3cf8-401a-bbe0-ef92139daac0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "mexico_city_features = pd.read_csv(\"./data/mexico-city-test-features.csv\")\n",
    "mexico_city_labels = pd.read_csv(\"./data/mexico-city-test-labels.csv\")\n",
    "X = mexico_city_features.surface_covered_in_m2.values.reshape(-1, 1)\n",
    "model_lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b421a13-6480-4f3c-acf7-f3516182ddc7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Accessing an Object in a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474b722-1170-4bbd-b6c1-49ef3e713363",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's figure out the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f35f09-362d-4955-8db3-f47dce8dea60",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "pipe.named_steps[\"regressor\"].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb12be-a863-4e72-90cf-4384e157a85b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Now obtain the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf20a90-6f71-45d8-8e65-44a12cd99b42",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# INCLUDE pipe.named_steps[...].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77caf70-121c-4309-8d4c-eb5f2ccaf5b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*References & Further Reading*\n",
    "- [One-Hot Encoding with the Category Encoder Package](https://contrib.scikit-learn.org/category_encoders/onehot.html)\n",
    "- [Example of Using One-Hot Encoding](https://scikit-learn.org/stable/auto_examples/linear_model/plot_tweedie_regression_insurance_claims.html#sphx-glr-auto-examples-linear-model-plot-tweedie-regression-insurance-claims-py)\n",
    "- [Online Example of Using One-Hot Encoding](https://stackabuse.com/one-hot-encoding-in-python-with-pandas-and-scikit-learn/)\n",
    "- [Official pandas Documentation on Get Dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\n",
    "- [Online Tutorial on Pipelines for Linear Regression](https://mahmoudyusof.github.io/general/scikit-learn-pipelines/)\n",
    "- [scikit-learn Pipeline Documentation](https://scikit-learn.org/stable/modules/compose.html#combining-estimators)\n",
    "- [Wikipedia article on the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_Learning)\n",
    "- [Wikipedia Article on Leakage in Machine Learning](https://en.wikipedia.org/wiki/Leakage_(machine_learning))\n",
    "- [Official Pandas Documentation on Missing Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n",
    "- [Wikipedia Article on Imputation](https://en.wikipedia.org/wiki/Imputation_(statistics))\n",
    "- [Online Tutorial on Removing Rows with Missing Data](https://datatofish.com/rows-with-nan-pandas-dataframe/)\n",
    "- [scikit-learn Documentation on `SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "- [imbalanced-learn Documentation](https://imbalanced-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd73d2-e513-4153-a668-baf799a8e1ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
