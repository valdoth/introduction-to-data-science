{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2f0196-ca43-4bfa-a8d2-cdfb38f004d7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+3\"><strong>Machine Learning: Unsupervised Learning</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78fef6-7eed-40e2-b444-9d0430b7c2ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Machine Learning falls into two classes, **supervised** learning and **unsupervised** learning. In supervised learning, we're trying to learn a predictive relationship between **features** of our data and some sort of  **target**. In unsupervised learning, we want to find trends in our features without using any target. \n",
    "\n",
    "A human example of supervised learning would be borrowing books from a library on mathematics and geography. By reading different books belonging to each topic, we learn what symbols, images, and words are associated with math, and which are associated with geography. A similar unsupervised task would be to borrow many books without knowing their subject. We can see some books contain similar images (maps) and some books contain similar symbols (e.g. the Greek letters $\\Sigma$ and $\\pi$). We say the books containing maps are similar and that they are different from the books containing Greek letters. Crucially, _we do not know what the books are about, only that they are similar or different_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f59b9-f71c-4d3d-9795-dabb62671244",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78fbc7-7768-437f-a9c6-cb3ce13f3938",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Clustering is a branch of unsupervised machine learning where the goal is to identify groups or clusters in your data set without the use of labels. Clustering should not be considered the same as classification. We are not trying make predictions on observations from a set of pre-defined classes. In clustering, you are identifying a set of similar data points and calling the resulting set a cluster.\n",
    "\n",
    "Let's consider an example of clustering. You may have a data set characterizing your customers like demographic information and personal preferences. A supervised machine learning task would be to predict which class a person belongs to: customers who will purchase your product and customers who won't. In contrast, an _unsupervised_ machine learning task would be to identify several groups or types of customers. With these groups identified, you can analyze the groups and build profiles describing the groups. For example, one group tends to include people from the ages 20 to 25 who like the outdoors. With these profiles, you can pass that information and analysis to the marketing team to create different advertisements to best attract each group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8136c-d78e-41b5-b70c-67356f461e68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## k-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc7cd5-2911-462b-ad5b-34becd6f4750",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The k-means algorithms seeks to find $K$ clusters within a data set. The clusters are chosen to reduce the distance of the data points within each cluster. The objective function is\n",
    "\n",
    "$$ \\min_{C_k} \\sum_{k} \\sum_{X_j \\in  C_k} \\| X_j  - \\mu_k \\|^2. $$\n",
    "\n",
    "where $\\mu_k$ is defined as the **centroid** of a cluster. Each cluster has a unique $\\mu_k$, which equals to the _mean_ of each feature/components of all points assigned to the cluster. We use the following equation to calculate $\\mu_k$:\n",
    "\n",
    "$$ \\mu_k = \\frac{1}{|C_k|} \\sum_{X_j \\in C_k} X_j, $$\n",
    "\n",
    "here $|C_k|$ is the number of points in cluster $k$. The training algorithm for k-means is iterative. First, we assign $k$ random starting locations as each cluster's centroid, then we:\n",
    "\n",
    "1. Assign each point to a cluster based on which cluster centroid it's closest to\n",
    "1. Calculate a new centroid for each cluster based its the datapoints\n",
    "1. Repeat the above steps until convergence is met\n",
    "\n",
    "To see how the algorithm works in practice, let's quickly go through this example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73415130-7a57-416b-a37c-27f5edb0fc62",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def wrangle(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    mask = df[\"TURNFEAR\"] == 1\n",
    "    df = df[mask]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Read Data into DataFrame\n",
    "df = wrangle(\"data/SCFP2019-textbook.csv.gz\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2a427-bbbe-426e-93d2-2e507d39d831",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the following example, we'll use `\"INCOME\"` and `\"HOUSES\"` to demonstrate the k-means clustering algorithm. First, we select the two features from our DataFrame as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b452c03-d6ea-488a-8804-b5ea9af227e8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "X = df[[\"INCOME\", \"HOUSES\"]]\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59d5bd-51e9-4f32-b1bf-4ef97bb50443",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next, we apply the k-means clustering algorithm from `sklearn`. We can choose the number of clusters to by defining `n_clusters`. In this example, we will show the results with 3 clusters. Note the algorithm starts with randomly assigned initial centroid positions, so defining a `random_state` will make sure the randomly assigned positions stay the same for repetitive runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4c64c-7eaf-41f5-a69f-71b9c0d1142a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "# Fit model to data\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7de270-b0d7-4cbc-bc04-3d3c1896f4ae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "After fitting the data, the model will assign each data point a `label`, indicating which cluster this data point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79121a3e-b23e-48cd-adb2-e6add772b5d5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "labels = model.labels_\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b594063-8487-4441-b0cd-eb5850d122ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To visualize the clustering results, we can quickly draw a scatter plot showing the two features. We can use different colors for different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7caf93a-c5bc-4a31-96cf-5152808e7736",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x=df[\"INCOME\"] / 1e6, y=df[\"HOUSES\"] / 1e6, hue=model.labels_, palette=\"deep\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Household Income [$1M]\")\n",
    "plt.ylabel(\"Home Value [$1M]\")\n",
    "plt.title(\"Home Value vs. Household Income\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa2513-73fc-48b1-9fd5-888a247c6a31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "From the scatter plot, we can see the k-means algorithm divides data points mostly based on the `HOUSE` feature. The lower home value data points are assigned to cluster 0, higher home value data points are assigned to cluster 1, while cluster 2 shows data points with home value in the middle.\n",
    "\n",
    "As mentioned in describing the k-means algorithm, **centroid** plays a very important role in deciding the clustering results. We can extract the final locations of centroid from each cluster, and plot them in the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a174c19-c46b-4d12-b80e-fa864fcc5f6f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Extract centroid\n",
    "centroids = model.cluster_centers_\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114667f-f6b8-4425-9936-b5f2910999e0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x=df[\"INCOME\"] / 1e6, y=df[\"HOUSES\"] / 1e6, hue=model.labels_, palette=\"deep\"\n",
    ")\n",
    "plt.scatter(\n",
    "    centroids[:, 0] / 1e6, centroids[:, 1] / 1e6, color=\"gray\", marker=\"*\", s=150\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Household Income [$1M]\")\n",
    "plt.ylabel(\"Home Value [$1M]\")\n",
    "plt.title(\"Home Value vs. Household Income\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6426661-53d3-4655-928a-a65871a84ec5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Clustering Metrics\n",
    "\n",
    "To see whether our clustering algorithm performs well, we need more than a scatter plot. The two common metrics we used are **inertia** and **silhouette score**. These metrics will also be helpful in determining the number of clusters to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d64e9e-59a1-4091-8d39-e773da86625d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78555357-97b1-425d-a1ba-a95f23d206af",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    " **Inertia** is the within-cluster sum of square distance, which is used in k-means algorithm's objective function. Mathematically, inertia is equal to\n",
    "\n",
    "$$ \\sum_{k} \\sum_{X_j \\in  C_k} \\| X_j  - \\mu_k \\|^2, $$\n",
    "\n",
    "where $\\mu_k$ is the centroid of cluster $k$ and $C_k$ is the set of points assigned to cluster $k$. Basically, the inertia is the sum of the distance of each point to the centroid or center of its assigned cluster. A lower inertia means the points assigned to the clusters are closer to the centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e91e1-8c45-47c2-aab7-1d5dccf70a9b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can extract `inertia` from the previous model using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcf33e-ce18-4f9e-89f0-6ef9622af9ac",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "inertia = model.inertia_\n",
    "print(\"Inertia (3 clusters):\", inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4760c2-7f99-423e-9538-f90f7e44bcfe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Silhouette Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eea7cf-6232-4533-a461-6837180ff661",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Silhouette Coefficient** is a measure of how dense and separated are the clusters. The silhouette coefficient is a property assigned to each data point. It's equal to\n",
    "\n",
    "$$ \\frac{b - a}{\\max(a, b)}, $$\n",
    "\n",
    "where $a$ is the distance between a point and centroid of its assigned cluster; $b$  is the distance between the point and the centroid of the nearest neighboring cluster (i.e. the closest cluster the point is not assigned to).\n",
    "\n",
    "The silhouette coefficient ranges from -1 to 1. If a point is really close to the centroid of its assigned cluster, then $a \\ll b$ and the silhouette coefficient will be approximately equal to 1. If the reverse is true, $a \\gg b$, then the coefficient will be -1. If the point could have been assigned to either cluster, its coefficient will be 0.\n",
    "\n",
    "Higher silhouette coefficient means higher density and highly separated clusters. This is because we want to have lower $a$ (close to assigned cluster's centroid) and higher $b$ (far away from unassigned cluster's centroid). A lower $a$ value combined with higher $b$ value will produce a higher silhouette score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5759498-0dba-40bf-9c59-47e50804fdd4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can extract calculate the silhouette score using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23689fb-72a5-431f-9cef-0a368ac91d2a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "ss = silhouette_score(X, model.labels_)\n",
    "print(\"Silhouette Score (3 clusters):\", ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9dc86-c166-4c3a-a131-fdfaa0115464",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Optimizing the Number of Clusters\n",
    "\n",
    "We can choose the optimal number of clusters by examining how number of cluster affect inertia and silhouette score. Let's check the following plots showing how inertia and silhouette scores change with respect to number of clusters ranging from 2 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fe6d1-5f6c-416d-99ac-31d4e86b16f0",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "n_clusters = range(2, 20)\n",
    "inertia_errors = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for n in n_clusters:\n",
    "    model = KMeans(n_clusters=n, random_state=42)\n",
    "    model.fit(X)\n",
    "    inertia_errors.append(model.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, model.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579835e-b7eb-46d7-96ce-6048749ab164",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we have saved the metrics, we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7015a0-fc49-4a38-8aee-d9ef0efb0fab",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "plt.plot(n_clusters, inertia_errors)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"k-means Model: Inertia vs Number of Clusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2a529-7303-429e-acb5-1a239ba46e46",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Note that inertia will decrease whenever the number of cluster increases. In fact, inertia will go to zero if the number of clusters equals number of data points, because each data point would be its own cluster. But that wouldn't make any practical sense, so we're not looking for the minimum point on the graph. Instead,we want the point where increasing numbers of clusters will not decrease inertia that much anymore. We usually refer to the point that indicating this change of inertia decreasing speed as the **elbow point**. In the example here, the elbow point is at 4-5.\n",
    "\n",
    "We can also plot the silhouette scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3a84c-0342-495c-8a81-3151fac7a95c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "plt.plot(n_clusters, silhouette_scores)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"k-means Model: Silhouette Score vs Number of Clusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a3e44-0ebe-4920-8859-97685899ec0c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "From the graph, we can see the silhouette score dropped a lot from 2 to 4, and from 5 to 6. Note that a higher silhouette score means denser and more clearly separated clusters, which is what we want. Combining both the inertia plot and the silhouette score plot, we can see the optimal number of cluster should be at 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c36188-1fc5-4b24-b1de-40635f3b422a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Use `ASSET` and `INCOME` from the same data set `\"SCFP2019-textbook.csv.gz\"`, and:\n",
    "\n",
    "1. Use k-means to assign the data points to 3 clusters.\n",
    "1. Create a scatter plot showing the resulting clusters and their centroids.\n",
    "1. Calculate the inertia and the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efdf913-aa02-4beb-96b8-bef7a7251a39",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "def wrangle(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    mask = df[\"TURNFEAR\"] == 1\n",
    "    df = df[mask]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Read Data into DataFrame\n",
    "df = ...\n",
    "\n",
    "# Select Features\n",
    "X = ...\n",
    "\n",
    "# Define Model\n",
    "model = ...\n",
    "\n",
    "# Fit model to data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000beb2d-70d5-49f4-a2d0-c534259afd29",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Get centroids\n",
    "centroids = ...\n",
    "\n",
    "# Plot \"ASSET\" vs \"HOUSES\" with hue=label\n",
    "\n",
    "\n",
    "plt.xlabel(\"Household Income [$1M]\")\n",
    "plt.ylabel(\"Home Value [$1M]\")\n",
    "plt.title(\"Home Value vs. ASSET\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25642c74-15bd-46f6-852b-85c5ab994c06",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "inertia = ...\n",
    "print(\"Inertia (3 clusters):\", inertia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2a7bc-5f10-471b-9fbd-18e79c2125f7",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "ss = ...\n",
    "print(\"Silhouette Score (3 clusters):\", ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b3b51-849c-4324-8367-c36ae3cb5759",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab457f3-0fc2-4eab-bf1d-8faeb373a432",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Principal component analysis (PCA) is a dimension reduction technique that takes a data set characterized by a set of possibly correlated features and generates a new set of features that are uncorrelated. It is used as a dimension reduction technique because the new set of uncorrelated features are chosen to be efficient in terms of capturing the variance in the data set.\n",
    "\n",
    "Let's examine a case where we have a data set of only two dimensions. In practice, PCA is rarely used when the dimension of the data set is already low. However, it is easier to illustrate the method when we have two or three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2faac9d-1ade-42ed-84cd-c5b882b70401",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "x1 = np.linspace(0, 1, 500)\n",
    "x2 = 2 * x1 + 1 + 0.2 * np.random.randn(500)\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "plt.scatter(*X.T, alpha=0.25)\n",
    "plt.plot(x1, 2 * x1 + 1, \"--k\", linewidth=2)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd27d252-9fa0-48af-a999-b19e19914d58",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The data plotted is characterized by two dimensions, but most of the variation does not occur in either of the two dimensions. Most of the points \"follow\" along the direction plotted in the dashed line. The variables $x_1$ and $x_2$ are highly correlated; as $x_1$ increases, in general, so does $x_2$ and vice versa.\n",
    "\n",
    "Instead of using the original two features, $x_1$ and $x_2$, perhaps we can use a different set of features, $\\xi_1$ and $\\xi_2$. The first chosen feature $\\xi_1$ should be aligned in the direction of greatest variation while the second will be _orthogonal_ to the first. The new axes/dimensions are referred to as _principal components_. Let's visualize the data set but using the principal components $\\xi_1$ and $\\xi_2$ rather than the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925d658-e190-4c83-81eb-32dc87becc24",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "Xt = pca.fit_transform(X)\n",
    "\n",
    "xi_1_max, xi_2_max = Xt.max(axis=0)\n",
    "xi_1_min, xi_2_min = Xt.min(axis=0)\n",
    "\n",
    "plt.hlines(0, xi_1_min, xi_1_max, linestyles=\"--\")\n",
    "plt.vlines(0, xi_2_min, xi_2_max, linestyles=\"--\")\n",
    "\n",
    "plt.scatter(*Xt.T, alpha=0.25)\n",
    "plt.xlim([-1.75, 1.75])\n",
    "plt.ylim([-1.75, 1.75])\n",
    "plt.xlabel(\"$\\\\xi _1$\")\n",
    "plt.ylabel(\"$\\\\xi _2$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04edbbd-ebe3-4394-8891-f9fcfc2e1e69",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the figure, we can clearly observe that $\\xi_1$ is the dimension with the largest variation. In the PCA algorithm, $\\xi_1$ is chosen to capture as much of the variation as possible, with $\\xi_2$ picking up the rest of remaining variation. Now, if we want to use one dimension to describe our data, we would keep $\\xi_1$ and drop $\\xi_2$, ensuring we keep as much of the information in our data set using just one dimension. Further, notice how the new dimensions are not correlated. As we move from lower to higher values of $\\xi_1$, $\\xi_2$ does not predictability increase or decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8374683-5642-49b8-9f7c-0c4dc57dce8f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## PCA in `scikit-learn`\n",
    "\n",
    "In `scikit-learn`, dimension reduction algorithms are transformers. The choice of having these algorithms as transformers makes sense since they apply a transformation on the data set. Let's illustrate the syntax for the PCA algorithm in `scikit-learn`. For most of these algorithms, the data needs to be centered and scaled to work properly. `PCA` automatically centers the data but **does not** scale it. `StandardScaler` is often used for preprocessing the data prior to applying PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193e36a-4053-431e-aef1-e98495c8dbd5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define trained data\n",
    "df = wrangle(\"data/SCFP2019-textbook.csv.gz\")\n",
    "X = df[[\"INCOME\", \"HOUSES\", \"DEBT\", \"NETWORTH\", \"NFIN\", \"ASSET\"]]\n",
    "\n",
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the model\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform data\n",
    "Xt = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Number of dimension before reduction:\", X_scaled.shape[-1])\n",
    "print(\"Number of dimension after reduction:\", Xt.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309bb58",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright Â© 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
